{
    "contents" : "---\ntitle: 'Machine learning: Do you do your exercises correctly?'\nauthor: \"Sergey Bushmanov\"\ndate: \"08/24/2014\"\noutput: \n  html_document:\n    toc: true\n    toc_depth: 2\n---\n## Executive summary\nPhysiologists and fitness industry have done a lot, quite validly, in promoting\nphysical activities. However, little attention has been paid so far to question:\nIf physical exercises performed \"correctly\"? This paper focuses on the machine\nlearning part of answering this question and shows how to turn movement data from\nsensors attached to athlete bodies to interpretable results.  \nThe trained model: `\"C5.0\"`  \nOut of sample accuracy on training set: 99.6%       \nCoursera accuracy on 20 sample test: 100%            \n\n## Note on compiling this document from .Rmd\n\nCompiling this document from scratch may take up to 10 hours, depending on computer,\ndue to computationally intensive 5-time 10-fold Cross-validation.\nThis is why I set global chunk options: `eval=FALSE`\nFully reproducible code can be found in github repo https://github.com/sbushmanov/Machine_Learning\nin well commented file `run-analysis.R`\n\n```{r setup}\nlibrary(knitr)\nopts_chunk$set(eval=FALSE)\n```\n\n\n\n## Data description\n\nThe data come from 4 accelerometers attached to: belt, forearm, arm, and dumbbell.\n6 individuals repeat 10 barbell lifts correctly and incorrectly under supervision\nof experienced trainer. Corresponding data from accelerometers is marked:  \n\n- \"A\", for correct execution  \n- \"B\", \"C\", \"D\", \"E\" for 4 different ways of incorrect execution  \n\nThe ultimate aim of this machine learning exercise, as stated above, is to train\ncategorical regression model to differentiate among 5 states of the world. The training data set resides at https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\nand testing data set at https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv. The progect page is http://groupware.les.inf.puc-rio.br/har.\n\n## Reading data into R  \n\nTo facilitate working with R code and save bandwidth I only download data if it has not been downloaded yet. \nGiven big size of data, the same logic applies to reading data as well:\n```{r}\nrequire(RCurl)\ntrainURL <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\ntestURL <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\nif (any(!file.exists(\"./data/train.csv\") | !file.exists(\"./data/train.csv\"))) {\n        dir.create(\"data\")\n        download.file(trainURL,\n                      destfile=\"./data/train.csv\",\n                      method = \"curl\")\n        download.file(testURL,\n                      destfile=\"./data/test.csv\",\n                      method = \"curl\")\n}\n\nif (any(!exists(\"train\") | !exists(\"test\"))) {\n        test <- read.csv(\"./data/test.csv\",\n                         header=TRUE,\n                         stringsAsFactors=F,\n                         na.strings=c(\"\", \"NA\", \"#DIV/0!\"))\n        train <- read.csv(\"./data/train.csv\",\n                          header=TRUE,\n                          stringsAsFactors=F,\n                          na.strings=c(\"\", \"NA\", \"#DIV/0!\"))\n}\n```\n\nNote `na.strings=c(\"#DIV/0!\")`. Without this otpion, when converting data to\nnumeric format, I was getting ~ 30 warnings. To find culprit's column I applied the\nfollowing trick:\n\n```{r eval=FALSE}\noptions(warn=2)\nfor (i in seq_along(t)) {\n        print(i)\n        t <- as.numeric(t[i])\n}\n```\nand found the column index where first warning originated. By visually inspecting \ncontents of that column I discovered this\nstrange `\"#DIV/0!\"` string. Adding it to two other `NA` codings eliminated warnings completely.\n\nThe next step is to extract predictors from columns.\nBy visual inspection of column name list `colnames(test)` and comparing column\nnames to accelerometer locations (belt, forearm, arm, and dumbbell), my conjecture at this step is that columns not containing these words can be dropped (this conjecture\nproved correct later by high Accuracy of the Final model).\n\n```{r}\nt <- train[grep(\"belt|forearm|arm|dumbbell|classe\", colnames(train))]\nte <- test[grep(\"belt|forearm|arm|dumbbell|problem_id\", colnames(test))]\n```\n\nNext step is to convert predictors to `numeric` format and outcomes to `factors`\n\n```{r}\nt1 <- as.data.frame(sapply(t[1:152], \"as.numeric\"))\nte1 <- as.data.frame(sapply(te[1:152], \"as.numeric\"))\nt1$classe <- factor(t$classe)\nte1$problem_id <- factor(te$problem_id)\n```\n\nObviously, not all accelerators are equal in predicting correctness of\nbarbell lifts (do you have an idea what belt accelerometer was for?). An\nelegant way of deciding what predictors contain most variability and decreasing number of\npredictors would be to apply SVD/PCA transformation, which would address multicollinearity\nas well. I have attempted brute-force by simply dropping near-zero variance\nvariables with the help of `caret` package:\n\n```{r}\nlibrary(caret)\nzeroVar <- nearZeroVar(t1)\nt2 <- t1[, -zeroVar]\nte2 <- te1[, -zeroVar]\n```\nAs I was getting some wierd warnings while trying to get working `svmRadial` and `gbm` models, I decided to get clean names for columns, while saving\nthem for later use in `t2Colnames` object.\n\n```{r}\nt2Colnames <- colnames(t2)\nt3 <- t2\nte3 <- te2\ncolnames(t3)[1:117] <- as.character(1:117)\ncolnames(te3)[1:117] <- as.character(1:117)\n```\n\n## Training model\n\nTraining R models on an IBM laptop  may take up to several days (was thinking about switching to NVIDIA CUDA once). Fixing laptop variable, the time depends\non type of model and the way model validation is applied.\n\n- **Type of model**. \n\nMax Kuhn et al in his  bestselling [\"Applied Predictive Modelling\"](http://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485/)\nprovides a breakup of categorical regression models by predictive ability,\n`C5.0`, `svm`, and `gbm` being among the best.   \n\nOn my machine `gbm` never finished simple training run, `svm` produced some incomprehensible warnings so\nI shelved it, and finally I decided to concentrate on `C5.0`.\n\n- **Cross-validation**\n\nTraining model on an entire training data set will (1) overfit model (2)\nprovide overoptimistic estimate for out of sample accuracy. To correct for\nthis bias, different techniques have been suggested: k-fold Cross-validation, \ngeneralized Cross-validation, Monte-Carlo Cross-validation, bootstrap, and\nleave-one out cross validation (k-fold Cross-validation with k set to sample size).\n\nFor this project I have chosen n-time k-fold cross validation, which may\nbe broken down into the following simple steps:  \n\n  - **Step 1.** Divide training set into k non-overlapping subsets (folds)  \n  - **Step 2.** Train on k-1 folds and estimate goodness of prediction on k-th (Accuracy, Kappa for\n  categorical regression; MSE/RMSE, R squared for numerical regression. May be ad-hoc, if you wish.). Do the same for all k folds.  \n  - **Step 3.** Repeat Steps 1 and 2 n times (to visualize what it means: imagine the data set is\n  a ring, divided into k-folds/strips. After training and validating on k-folds at hand, you\n  shift division along ring to obtain new set of k-folds and repeat shifting n-times)\n\nWith the help of `caret` package 5-time 10-fold Cross-validation is coded\nas follows:\n\n```{r}\nctrl <- trainControl(method = \"repeatedcv\",\n                     repeats = 5,\n                     number = 10)\n```\n\nand the final model becomes:\n```{r eval=FALSE}\nlibrary(doMC)\nregisterDoMC(cores = 2)\nc50TuneFinal <- train(x =t3[, -118],\n                      y=t3[,118],\n                      method = \"C5.0\",\n                      preProc = c(\"center\", \"scale\"),\n                      trControl = ctrl)\n```\n\nPlease note that n-repeats k-fold Cross validation is computationally intensive\nprocedure. 5-time 10-fold Cross validation increase computation time by ~50 times\nover one simple regression fit. This is why I turn on multithreading in attempt to reduce\ncomputation time.\n\nModel summary:\n\n```{r}\n> c50TuneFinal\nC5.0 \n\n19622 samples\n  117 predictors\n    5 classes: 'A', 'B', 'C', 'D', 'E' \n\nPre-processing: centered, scaled \nResampling: Cross-Validated (10 fold, repeated 5 times) \n\nSummary of sample sizes: 17660, 17658, 17659, 17660, 17661, 17659, ... \n\nResampling results across tuning parameters:\n\n  model  winnow  trials  Accuracy  Kappa  Accuracy SD  Kappa SD\n  rules  FALSE    1      0.971     0.963  0.00510      0.00645 \n  rules  FALSE   10      0.995     0.993  0.00202      0.00255 \n  rules  FALSE   20      0.996     0.995  0.00131      0.00166 \n  rules   TRUE    1      0.971     0.964  0.00531      0.00672 \n  rules   TRUE   10      0.995     0.994  0.00183      0.00231 \n  rules   TRUE   20      0.996     0.996  0.00162      0.00205 \n  tree   FALSE    1      0.967     0.959  0.00573      0.00726 \n  tree   FALSE   10      0.994     0.993  0.00152      0.00192 \n  tree   FALSE   20      0.996     0.995  0.00138      0.00175 \n  tree    TRUE    1      0.967     0.958  0.00542      0.00685 \n  tree    TRUE   10      0.994     0.992  0.00182      0.00231 \n  tree    TRUE   20      0.996     0.995  0.00157      0.00198 \n\nAccuracy was used to select the optimal model using  the largest value.\nThe final values used for the model were trials = 20, model = rules and winnow = TRUE. \n```\n\nEstimated out-of-sample accuracy for Final Model as per Project assignment question is `0.996`\n\nModel prediction:\n```{r }\nanswers <- predict(c50TuneFinal, newdata=te3[,-118])\n```\n\n\nScreenshot of the Cousera 20-sample test:\n![20-sample-blind-test](screenshot/RStudio_screen1.png)\n\nThe time training model took is :\n\n```{r}\nc50TuneFinal$times$everything\n```\n\n```{r eval=FALSE}\n> c50TuneFinal$times$everything\n     user    system   elapsed \n 9375.510    19.044 10578.738 \n```\n\n\n## Reproducibility\n1. Github repo https://github.com/sbushmanov/Machine_Learning contains following files:  \n  - `run-analysis.R` -- code to reproduce results presented in this document\n  - `index.Rmd` -- `.Rmd` source of this document\n  - `index.md`, `index.html` -- compiled versions of this document\n2. http://sbushmanov.github.io/machine_learning is the address for `gh-page` of this document\n\n\n## Data credit\n\nVelloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.\n\nRead more: http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises#ixzz3BKm9xveF",
    "created" : 1408907507966.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "937419461",
    "id" : "AED351EF",
    "lastKnownWriteTime" : 1408907100,
    "path" : "/home/sergey/R_COURSERA_Machine_Learning_Project/index.Rmd",
    "project_path" : "index.Rmd",
    "properties" : {
        "tempName" : "Untitled3"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}